{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T10:36:44.021324Z",
     "start_time": "2024-09-11T10:36:44.018915Z"
    }
   },
   "source": [
    "# !pip install gradio numpy torch transformers nltk\n",
    "# !pip install accelerate\n",
    "# !pip install bitsandbytes\n",
    "# !pip install typer==0.12\n",
    "# !pip install spacy==3.7.4\n",
    "# !pip install weasel==0.3.4"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T10:38:56.483041Z",
     "start_time": "2024-09-11T10:36:44.736807Z"
    }
   },
   "source": [
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "model_name = \"yam-peleg/Hebrew-Mistral-7B\"\n",
    "cache_dir = \"hebrew_mistral_cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir, quantization_config=quantization_config)\n",
    "\n",
    "def generate_response(input_text, max_new_tokens, min_length, no_repeat_ngram_size, num_beams, early_stopping, temperature, top_p, top_k):\n",
    "   input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "   outputs = model.generate(\n",
    "       **input_ids,\n",
    "       max_new_tokens=max_new_tokens,\n",
    "       min_length=min_length,\n",
    "       no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "       num_beams=num_beams,\n",
    "       early_stopping=early_stopping,\n",
    "       temperature=temperature,\n",
    "       top_p=top_p,\n",
    "       top_k=top_k,\n",
    "       pad_token_id=tokenizer.eos_token_id,\n",
    "       do_sample=True\n",
    "   )\n",
    "   response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "   return response\n",
    "\n",
    "def create_paragraphs(bot_response, sentences_per_paragraph=4):\n",
    "   sentences = sent_tokenize(bot_response)\n",
    "   paragraphs = []\n",
    "   current_paragraph = \"\"\n",
    "\n",
    "   for i, sentence in enumerate(sentences, start=1):\n",
    "       current_paragraph += \" \" + sentence\n",
    "       if i % sentences_per_paragraph == 0:\n",
    "           paragraphs.append(current_paragraph.strip())\n",
    "           current_paragraph = \"\"\n",
    "\n",
    "   if current_paragraph:\n",
    "       paragraphs.append(current_paragraph.strip())\n",
    "\n",
    "   formatted_paragraphs = \"\\n\".join([f'<p style=\"text-align: right; direction: rtl;\">{p}</p>' for p in paragraphs])\n",
    "   return formatted_paragraphs\n",
    "\n",
    "def remove_paragraphs(text):\n",
    "   return text.replace(\"\\n\", \" \")\n",
    "\n",
    "def copy_last_response(history):\n",
    "    if history:\n",
    "        last_response = history[-1][1]\n",
    "        last_response = last_response.replace('<div style=\"text-align: right; direction: rtl;\">', '').replace('</div>', '')\n",
    "        last_response = last_response.replace('<p style=\"text-align: right; direction: rtl;\">', '').replace('</p>', '')\n",
    "        last_response = last_response.replace('\\n', ' ')\n",
    "        return last_response\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def chat(input_text, history, max_new_tokens, min_length, no_repeat_ngram_size, num_beams, early_stopping, temperature, top_p, top_k, create_paragraphs_enabled):\n",
    "   user_input = f'<div style=\"text-align: right; direction: rtl;\">{input_text}</div>'\n",
    "   response = generate_response(input_text, max_new_tokens, min_length, no_repeat_ngram_size, num_beams, early_stopping, temperature, top_p, top_k)\n",
    "\n",
    "   if create_paragraphs_enabled:\n",
    "       response = create_paragraphs(response)\n",
    "\n",
    "   bot_response = f'<div style=\"text-align: right; direction: rtl;\">{response}</div>'\n",
    "   history.append((user_input, bot_response))\n",
    "\n",
    "   return history, history, input_text\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "   gr.Markdown(\"# Hebrew-Mistral-7B Instract-bot\", elem_id=\"title\")\n",
    "   gr.Markdown(\"Model by Yam Peleg | GUI by Shmuel Ronen\", elem_id=\"subtitle\")\n",
    "   \n",
    "   chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
    "   \n",
    "   with gr.Row():\n",
    "       message = gr.Textbox(placeholder=\"Type your message...\", label=\"User\", elem_id=\"message\")\n",
    "       submit = gr.Button(\"Send\")\n",
    "\n",
    "   with gr.Row():\n",
    "       create_paragraphs_checkbox = gr.Checkbox(label=\"Create Paragraphs\", value=False)\n",
    "       remove_paragraphs_btn = gr.Button(\"Remove Paragraphs\")\n",
    "       copy_last_btn = gr.Button(\"Copy Last Response\")\n",
    "   \n",
    "   with gr.Accordion(\"Adjustments\", open=False):\n",
    "       with gr.Row():    \n",
    "           with gr.Column():\n",
    "               max_new_tokens = gr.Slider(minimum=10, maximum=1500, value=100, step=10, label=\"Max New Tokens\")\n",
    "               min_length = gr.Slider(minimum=10, maximum=300, value=100, step=10, label=\"Min Length\")\n",
    "               no_repeat_ngram_size = gr.Slider(minimum=1, maximum=6, value=4, step=1, label=\"No Repeat N-Gram Size\")\n",
    "           with gr.Column():\n",
    "               num_beams = gr.Slider(minimum=1, maximum=16, value=4, step=1, label=\"Num Beams\") \n",
    "               temperature = gr.Slider(minimum=0.1, maximum=2.0, value=0.2, step=0.1, label=\"Temperature\")\n",
    "               top_p = gr.Slider(minimum=0.1, maximum=1.0, value=0.7, step=0.1, label=\"Top P\")\n",
    "               top_k = gr.Slider(minimum=1, maximum=100, value=30, step=1, label=\"Top K\")\n",
    "       early_stopping = gr.Checkbox(value=True, label=\"Early Stopping\")\n",
    "   \n",
    "   submit.click(chat, inputs=[message, chatbot, max_new_tokens, min_length, no_repeat_ngram_size, num_beams, early_stopping, temperature, top_p, top_k, create_paragraphs_checkbox], outputs=[chatbot, chatbot, message])\n",
    "   remove_paragraphs_btn.click(remove_paragraphs, inputs=message, outputs=message)\n",
    "   copy_last_btn.click(copy_last_response, inputs=chatbot, outputs=message)\n",
    "   \n",
    "   demo.css = \"\"\"\n",
    "       #message, #message * {\n",
    "           text-align: right !important;\n",
    "           direction: rtl !important;\n",
    "       }\n",
    "       \n",
    "       #chatbot, #chatbot * {\n",
    "           text-align: right !important;\n",
    "           direction: rtl !important;\n",
    "       }\n",
    "       \n",
    "       #title, .label {\n",
    "           text-align: right !important;\n",
    "       }\n",
    "       \n",
    "       #subtitle {\n",
    "           text-align: left !important;\n",
    "       }\n",
    "   \"\"\"\n",
    "\n",
    "demo.launch()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yuda/nltk_data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenize\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sent_tokenize\n\u001B[0;32m----> 8\u001B[0m nltk\u001B[38;5;241m.\u001B[39mdownload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpunkt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     10\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myam-peleg/Hebrew-Mistral-7B\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     11\u001B[0m cache_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhebrew_mistral_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/downloader.py:777\u001B[0m, in \u001B[0;36mDownloader.download\u001B[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001B[0m\n\u001B[1;32m    768\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshow\u001B[39m(s, prefix2\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    769\u001B[0m     print_to(\n\u001B[1;32m    770\u001B[0m         textwrap\u001B[38;5;241m.\u001B[39mfill(\n\u001B[1;32m    771\u001B[0m             s,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    774\u001B[0m         )\n\u001B[1;32m    775\u001B[0m     )\n\u001B[0;32m--> 777\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m msg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mincr_download(info_or_id, download_dir, force):\n\u001B[1;32m    778\u001B[0m     \u001B[38;5;66;03m# Error messages\u001B[39;00m\n\u001B[1;32m    779\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(msg, ErrorMessage):\n\u001B[1;32m    780\u001B[0m         show(msg\u001B[38;5;241m.\u001B[39mmessage)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/downloader.py:642\u001B[0m, in \u001B[0;36mDownloader.incr_download\u001B[0;34m(self, info_or_id, download_dir, force)\u001B[0m\n\u001B[1;32m    638\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m FinishCollectionMessage(info)\n\u001B[1;32m    640\u001B[0m \u001B[38;5;66;03m# Handle Packages (delegate to a helper function).\u001B[39;00m\n\u001B[1;32m    641\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 642\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_download_package(info, download_dir, force)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/downloader.py:708\u001B[0m, in \u001B[0;36mDownloader._download_package\u001B[0;34m(self, info, download_dir, force)\u001B[0m\n\u001B[1;32m    706\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m ProgressMessage(\u001B[38;5;241m5\u001B[39m)\n\u001B[1;32m    707\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 708\u001B[0m     infile \u001B[38;5;241m=\u001B[39m urlopen(info\u001B[38;5;241m.\u001B[39murl)\n\u001B[1;32m    709\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(filepath, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m outfile:\n\u001B[1;32m    710\u001B[0m         num_blocks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;241m1\u001B[39m, info\u001B[38;5;241m.\u001B[39msize \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1024\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m16\u001B[39m))\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/urllib/request.py:215\u001B[0m, in \u001B[0;36murlopen\u001B[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    214\u001B[0m     opener \u001B[38;5;241m=\u001B[39m _opener\n\u001B[0;32m--> 215\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m opener\u001B[38;5;241m.\u001B[39mopen(url, data, timeout)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/urllib/request.py:515\u001B[0m, in \u001B[0;36mOpenerDirector.open\u001B[0;34m(self, fullurl, data, timeout)\u001B[0m\n\u001B[1;32m    512\u001B[0m     req \u001B[38;5;241m=\u001B[39m meth(req)\n\u001B[1;32m    514\u001B[0m sys\u001B[38;5;241m.\u001B[39maudit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124murllib.Request\u001B[39m\u001B[38;5;124m'\u001B[39m, req\u001B[38;5;241m.\u001B[39mfull_url, req\u001B[38;5;241m.\u001B[39mdata, req\u001B[38;5;241m.\u001B[39mheaders, req\u001B[38;5;241m.\u001B[39mget_method())\n\u001B[0;32m--> 515\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_open(req, data)\n\u001B[1;32m    517\u001B[0m \u001B[38;5;66;03m# post-process response\u001B[39;00m\n\u001B[1;32m    518\u001B[0m meth_name \u001B[38;5;241m=\u001B[39m protocol\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_response\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/urllib/request.py:532\u001B[0m, in \u001B[0;36mOpenerDirector._open\u001B[0;34m(self, req, data)\u001B[0m\n\u001B[1;32m    529\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[1;32m    531\u001B[0m protocol \u001B[38;5;241m=\u001B[39m req\u001B[38;5;241m.\u001B[39mtype\n\u001B[0;32m--> 532\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_chain(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_open, protocol, protocol \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m    533\u001B[0m                           \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_open\u001B[39m\u001B[38;5;124m'\u001B[39m, req)\n\u001B[1;32m    534\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result:\n\u001B[1;32m    535\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/urllib/request.py:492\u001B[0m, in \u001B[0;36mOpenerDirector._call_chain\u001B[0;34m(self, chain, kind, meth_name, *args)\u001B[0m\n\u001B[1;32m    490\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m handler \u001B[38;5;129;01min\u001B[39;00m handlers:\n\u001B[1;32m    491\u001B[0m     func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[0;32m--> 492\u001B[0m     result \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m    493\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    494\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/urllib/request.py:1392\u001B[0m, in \u001B[0;36mHTTPSHandler.https_open\u001B[0;34m(self, req)\u001B[0m\n\u001B[1;32m   1391\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhttps_open\u001B[39m(\u001B[38;5;28mself\u001B[39m, req):\n\u001B[0;32m-> 1392\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_open(http\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mHTTPSConnection, req,\n\u001B[1;32m   1393\u001B[0m                         context\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_context)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/urllib/request.py:1344\u001B[0m, in \u001B[0;36mAbstractHTTPHandler.do_open\u001B[0;34m(self, http_class, req, **http_conn_args)\u001B[0m\n\u001B[1;32m   1342\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1343\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1344\u001B[0m         h\u001B[38;5;241m.\u001B[39mrequest(req\u001B[38;5;241m.\u001B[39mget_method(), req\u001B[38;5;241m.\u001B[39mselector, req\u001B[38;5;241m.\u001B[39mdata, headers,\n\u001B[1;32m   1345\u001B[0m                   encode_chunked\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39mhas_header(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTransfer-encoding\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m   1346\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n\u001B[1;32m   1347\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m URLError(err)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1336\u001B[0m, in \u001B[0;36mHTTPConnection.request\u001B[0;34m(self, method, url, body, headers, encode_chunked)\u001B[0m\n\u001B[1;32m   1333\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\u001B[38;5;28mself\u001B[39m, method, url, body\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, headers\u001B[38;5;241m=\u001B[39m{}, \u001B[38;5;241m*\u001B[39m,\n\u001B[1;32m   1334\u001B[0m             encode_chunked\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1335\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1336\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1382\u001B[0m, in \u001B[0;36mHTTPConnection._send_request\u001B[0;34m(self, method, url, body, headers, encode_chunked)\u001B[0m\n\u001B[1;32m   1378\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(body, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m   1379\u001B[0m     \u001B[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001B[39;00m\n\u001B[1;32m   1380\u001B[0m     \u001B[38;5;66;03m# default charset of iso-8859-1.\u001B[39;00m\n\u001B[1;32m   1381\u001B[0m     body \u001B[38;5;241m=\u001B[39m _encode(body, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m-> 1382\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mendheaders(body, encode_chunked\u001B[38;5;241m=\u001B[39mencode_chunked)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1331\u001B[0m, in \u001B[0;36mHTTPConnection.endheaders\u001B[0;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[1;32m   1329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CannotSendHeader()\n\u001B[0;32m-> 1331\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_send_output(message_body, encode_chunked\u001B[38;5;241m=\u001B[39mencode_chunked)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1091\u001B[0m, in \u001B[0;36mHTTPConnection._send_output\u001B[0;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[1;32m   1089\u001B[0m msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer)\n\u001B[1;32m   1090\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer[:]\n\u001B[0;32m-> 1091\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(msg)\n\u001B[1;32m   1093\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m message_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1094\u001B[0m \n\u001B[1;32m   1095\u001B[0m     \u001B[38;5;66;03m# create a consistent interface to message_body\u001B[39;00m\n\u001B[1;32m   1096\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(message_body, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mread\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m   1097\u001B[0m         \u001B[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001B[39;00m\n\u001B[1;32m   1098\u001B[0m         \u001B[38;5;66;03m# is needed to allow the current position of mmap'ed\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m         \u001B[38;5;66;03m# files to be taken into account.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1035\u001B[0m, in \u001B[0;36mHTTPConnection.send\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m   1033\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1034\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_open:\n\u001B[0;32m-> 1035\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconnect()\n\u001B[1;32m   1036\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1037\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m NotConnected()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1470\u001B[0m, in \u001B[0;36mHTTPSConnection.connect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1467\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconnect\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m   1468\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnect to a host on a given (SSL) port.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1470\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mconnect()\n\u001B[1;32m   1472\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tunnel_host:\n\u001B[1;32m   1473\u001B[0m         server_hostname \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tunnel_host\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1001\u001B[0m, in \u001B[0;36mHTTPConnection.connect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    999\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001B[39;00m\n\u001B[1;32m   1000\u001B[0m sys\u001B[38;5;241m.\u001B[39maudit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttp.client.connect\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mport)\n\u001B[0;32m-> 1001\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection(\n\u001B[1;32m   1002\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost,\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mport), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_address)\n\u001B[1;32m   1003\u001B[0m \u001B[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001B[39;00m\n\u001B[1;32m   1004\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/socket.py:838\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, all_errors)\u001B[0m\n\u001B[1;32m    836\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m source_address:\n\u001B[1;32m    837\u001B[0m     sock\u001B[38;5;241m.\u001B[39mbind(source_address)\n\u001B[0;32m--> 838\u001B[0m sock\u001B[38;5;241m.\u001B[39mconnect(sa)\n\u001B[1;32m    839\u001B[0m \u001B[38;5;66;03m# Break explicitly a reference cycle\u001B[39;00m\n\u001B[1;32m    840\u001B[0m exceptions\u001B[38;5;241m.\u001B[39mclear()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T10:42:16.242559Z",
     "start_time": "2024-09-11T10:39:37.464408Z"
    }
   },
   "cell_type": "code",
   "source": "!python heb.py\n",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yuda/nltk_data...\r\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\r\n",
      "tokenizer_config.json: 100%|███████████████| 1.13k/1.13k [00:00<00:00, 1.88MB/s]\r\n",
      "tokenizer.model: 100%|█████████████████████| 1.08M/1.08M [00:00<00:00, 4.24MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 4.33M/4.33M [00:00<00:00, 6.48MB/s]\r\n",
      "added_tokens.json: 100%|██████████████████████| 21.0/21.0 [00:00<00:00, 144kB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 552/552 [00:00<00:00, 3.58MB/s]\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/yuda/PycharmProjects/LLM_prompt_server/Hebrew-Mistral-7B-GUI/heb.py\", line 14, in <module>\r\n",
      "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\r\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py\", line 897, in from_pretrained\r\n",
      "    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 2271, in from_pretrained\r\n",
      "    return cls._from_pretrained(\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 2505, in _from_pretrained\r\n",
      "    tokenizer = cls(*init_inputs, **init_kwargs)\r\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/models/llama/tokenization_llama_fast.py\", line 157, in __init__\r\n",
      "    super().__init__(\r\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py\", line 106, in __init__\r\n",
      "    raise ValueError(\r\n",
      "ValueError: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.\r\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
